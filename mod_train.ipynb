{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available() # NOTE: This should be True for everything to work smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "from transformers import (\n",
    "    TrainerCallback,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model, PeftModel, LoraConfig, LoftQConfig\n",
    "import torch\n",
    "from peft.tuners.lora import LoraLayer\n",
    "import os\n",
    "from functools import wraps\n",
    "import random\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHAT_TEMPLATE= \"\"\"{% for message in messages %}\n",
    "    {% if message['role'] == 'user' %}\n",
    "        {{'<user> ' + message['content'].strip() + ' </user>' }}\n",
    "    {% elif message['role'] == 'system' %}\n",
    "        {{'<system>\\\\n' + message['content'].strip() + '\\\\n</system>\\\\n\\\\n' }}\n",
    "    {% elif message['role'] == 'assistant' %}\n",
    "        {{ message['content'].strip() + ' </assistant>' + eos_token }}\n",
    "    {% elif message['role'] == 'input' %}\n",
    "        {{'<input> ' + message['content'] + ' </input>' }}\n",
    "    {% endif %}\n",
    "{% endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QLoraWrapperModelInit:    \n",
    "    \"\"\"\n",
    "    A wrapper class for initializing transformer-based models with QLoRa and gradient checkpointing.\n",
    "\n",
    "    This class serves as a wrapper for the `model_init` function, which initializes the model.\n",
    "    It activates gradient checkpointing when possible and applies QLoRa to the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_init : callable\n",
    "        A function that initializes the transformer-based model for training.\n",
    "    model_config : Any\n",
    "        The configuration for the model.\n",
    "    tokenizer : Any\n",
    "        The tokenizer used for tokenization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pre-trained model with QLoRa and gradient checkpointing, if enabled.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_init: Any, model_config: Any, tokenizer: Any) -> None:\n",
    "        self.model_init = model_init\n",
    "        self.model_config = model_config\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self) -> PreTrainedModel:\n",
    "        \"\"\"\n",
    "        Initialize the model and apply QLoRa and gradient checkpointing when configured.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Pre-trained model with QLoRa and gradient checkpointing, if enabled.\n",
    "        \"\"\"\n",
    "        model = self.model_init()\n",
    "        has_gradient_checkpointing = False\n",
    "        if not model.__class__.__name__ in [\n",
    "            \"MPTForCausalLM\",\n",
    "            \"MixFormerSequentialForCausalLM\",\n",
    "        ]:\n",
    "            try:\n",
    "                model.resize_token_embeddings(len(self.tokenizer))\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Could not resize token embeddings due to {e}, but will continue anyway...\"\n",
    "                )\n",
    "            try:\n",
    "                model.gradient_checkpointing_enable()\n",
    "                has_gradient_checkpointing = True\n",
    "            except Exception as e:\n",
    "                print(f\"Model checkpointing did not work: {e}\")\n",
    "        if model.__class__.__name__ == \"LlamaForCausalLM\":\n",
    "            model.config.pretraining_tp = 1\n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model, use_gradient_checkpointing=has_gradient_checkpointing\n",
    "        )\n",
    "        model = get_peft_model(model, self.model_config.peft_config)\n",
    "        model.config.use_cache = False\n",
    "        if self.model_config.neftune_noise_alpha is not None:\n",
    "            model = activate_neftune(model, self.model_config.neftune_noise_alpha)\n",
    "        model = self.change_layer_types_for_stability(model)\n",
    "        return model\n",
    "\n",
    "    def change_layer_types_for_stability(\n",
    "        self, model: PreTrainedModel\n",
    "    ) -> PreTrainedModel:\n",
    "        \"\"\"\n",
    "        Change layer types of the model for stability.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : PreTrainedModel\n",
    "            The pre-trained model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Pre-trained model with modified layer types for stability.\n",
    "        \"\"\"\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, LoraLayer):\n",
    "                module = module.to(torch.float32)\n",
    "            if \"norm\" in name:\n",
    "                module = module.to(torch.float32)\n",
    "            if \"lm_head\" in name or \"embed_tokens\" in name:\n",
    "                if hasattr(module, \"weight\"):\n",
    "                    module = module.to(torch.float32)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autotransformers import AutoTrainer, DatasetConfig, ModelConfig\n",
    "from autotransformers.llm_templates import instructions_to_chat, NEFTuneTrainer, modify_tokenizer, SavePeftModelCallback\n",
    "from functools import partial\n",
    "from peft import LoraConfig, LoftQConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"somosnlp/Conversaciones_terapeuticas_espanol\", split= \"train\")\n",
    "ds = ds.rename_column(\"chat\", \"messages\")\n",
    "ds = ds.train_test_split(test_size=0.1)\n",
    "ds = ds[\"train\"].train_test_split(0.2, seed=203984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fixed_train_args = {\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"fp16\": True,  # Cambiado de True a False\n",
    "    \"logging_steps\": 50,\n",
    "    \"lr_scheduler_type\": \"constant\",\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"eval_steps\": 200,\n",
    "    \"save_steps\": 50,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"logging_first_step\": True,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"max_grad_norm\": 0.3,\n",
    "    \"optim\": \"paged_adamw_32bit\",\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"group_by_length\": False,\n",
    "    \"save_total_limit\": 50,\n",
    "    \"adam_beta2\": 0.999\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config = {\n",
    "        \"seed\": 9834,\n",
    "        \"direction_optimize\": \"minimize\",\n",
    "        \"metric_optimize\": \"eval_loss\",\n",
    "        \"callbacks\": [SavePeftModelCallback],\n",
    "        \"fixed_training_args\": fixed_train_args,\n",
    "        \"dataset_name\": \"Conversaciones_terapeuticas_espanol\",\n",
    "        \"alias\": \"terapia\",\n",
    "        \"retrain_at_end\": False,\n",
    "        \"task\": \"chatbot\",\n",
    "        \"text_field\": \"messages\",\n",
    "        \"label_col\": \"messages\",\n",
    "        \"num_proc\": 4,\n",
    "        \"loaded_dataset\": ds,\n",
    "        \"partial_split\": True, # to create a validation split.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config = DatasetConfig(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=256,\n",
    "        lora_alpha=32,\n",
    "        target_modules=\"all-linear\",  # \"query_key_value\" # \"Wqkv\"\n",
    "        lora_dropout=0.1,  # 0.1 for <13B models, 0.05 otherwise.\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        use_rslora=True,\n",
    "        loftq_config=LoftQConfig(loftq_bits=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "funciona_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_config = ModelConfig(\n",
    "    name=\"google/gemma-2b-it\",\n",
    "    save_name=\"gemma_2b\",\n",
    "    save_dir=\"./gemma_2b_terapia_1\",\n",
    "    custom_params_model={\"trust_remote_code\": True, \"device_map\": {\"\": 0}},\n",
    "    model_init_wrap_cls=QLoraWrapperModelInit,\n",
    "    quantization_config=funciona_config,\n",
    "    peft_config=lora_config,\n",
    "    # neftune_noise_alpha=10,\n",
    "    # custom_trainer_cls=NEFTuneTrainer,\n",
    "    func_modify_tokenizer=partial(\n",
    "        modify_tokenizer,\n",
    "        new_model_seq_length=4096, # lower the maximum seq length to 4096 instead of 8192 to fit in google colab GPUs.\n",
    "        add_special_tokens={\"pad_token\": \"[PAD]\"}, # add pad token.\n",
    "        chat_template=CHAT_TEMPLATE # add the new chat template including the system and input roles.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autotrainer = AutoTrainer(\n",
    "    model_configs=[gemma_config],\n",
    "    dataset_configs=[model_config],\n",
    "    metrics_dir=\"./chaterapia\",\n",
    "    hp_search_mode=\"fixed\",\n",
    "    clean=True,\n",
    "    metrics_cleaner=\"tmp_metrics_cleaner\",\n",
    "    use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = autotrainer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
