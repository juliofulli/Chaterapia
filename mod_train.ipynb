{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/lenguajenatural-ai/autotransformers/blob/master/notebooks/chatbot_instructions/train_instructional_chatbot.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "pip install huggingface_hub\n",
    "pip install torch\n",
    "pip install autotransformers && pip install transformers==4.38.2 && pip install -U peft\n",
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.8\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f224127671e84165952b742ca31ad519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available() # NOTE: This should be True for everything to work smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHAT_TEMPLATE= \"\"\"{% for message in messages %}\n",
    "    {% if message['role'] == 'user' %}\n",
    "        {{'<user> ' + message['content'].strip() + ' </user>' }}\n",
    "    {% elif message['role'] == 'system' %}\n",
    "        {{'<system>\\\\n' + message['content'].strip() + '\\\\n</system>\\\\n\\\\n' }}\n",
    "    {% elif message['role'] == 'assistant' %}\n",
    "        {{ message['content'].strip() + ' </assistant>' + eos_token }}\n",
    "    {% elif message['role'] == 'input' %}\n",
    "        {{'<input> ' + message['content'] + ' </input>' }}\n",
    "    {% endif %}\n",
    "{% endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ab50e3e93243818714571dbeee5971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Any, Optional\n",
    "from transformers import (\n",
    "    TrainerCallback,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model, PeftModel\n",
    "import torch\n",
    "from peft.tuners.lora import LoraLayer\n",
    "import os\n",
    "from functools import wraps\n",
    "import random\n",
    "class QLoraWrapperModelInit:\n",
    "    \"\"\"\n",
    "    A wrapper class for initializing transformer-based models with QLoRa and gradient checkpointing.\n",
    "\n",
    "    This class serves as a wrapper for the `model_init` function, which initializes the model.\n",
    "    It activates gradient checkpointing when possible and applies QLoRa to the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_init : callable\n",
    "        A function that initializes the transformer-based model for training.\n",
    "    model_config : Any\n",
    "        The configuration for the model.\n",
    "    tokenizer : Any\n",
    "        The tokenizer used for tokenization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pre-trained model with QLoRa and gradient checkpointing, if enabled.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_init: Any, model_config: Any, tokenizer: Any) -> None:\n",
    "        self.model_init = model_init\n",
    "        self.model_config = model_config\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self) -> PreTrainedModel:\n",
    "        \"\"\"\n",
    "        Initialize the model and apply QLoRa and gradient checkpointing when configured.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Pre-trained model with QLoRa and gradient checkpointing, if enabled.\n",
    "        \"\"\"\n",
    "        model = self.model_init()\n",
    "        has_gradient_checkpointing = False\n",
    "        if not model.__class__.__name__ in [\n",
    "            \"MPTForCausalLM\",\n",
    "            \"MixFormerSequentialForCausalLM\",\n",
    "        ]:\n",
    "            try:\n",
    "                model.resize_token_embeddings(len(self.tokenizer))\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Could not resize token embeddings due to {e}, but will continue anyway...\"\n",
    "                )\n",
    "            try:\n",
    "                model.gradient_checkpointing_enable()\n",
    "                has_gradient_checkpointing = True\n",
    "            except Exception as e:\n",
    "                print(f\"Model checkpointing did not work: {e}\")\n",
    "        if model.__class__.__name__ == \"LlamaForCausalLM\":\n",
    "            model.config.pretraining_tp = 1\n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model, use_gradient_checkpointing=has_gradient_checkpointing\n",
    "        )\n",
    "        model = get_peft_model(model, self.model_config.peft_config)\n",
    "        model.config.use_cache = False\n",
    "        if self.model_config.neftune_noise_alpha is not None:\n",
    "            model = activate_neftune(model, self.model_config.neftune_noise_alpha)\n",
    "        model = self.change_layer_types_for_stability(model)\n",
    "        return model\n",
    "\n",
    "    def change_layer_types_for_stability(\n",
    "        self, model: PreTrainedModel\n",
    "    ) -> PreTrainedModel:\n",
    "        \"\"\"\n",
    "        Change layer types of the model for stability.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : PreTrainedModel\n",
    "            The pre-trained model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Pre-trained model with modified layer types for stability.\n",
    "        \"\"\"\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, LoraLayer):\n",
    "                module = module.to(torch.float32)\n",
    "            if \"norm\" in name:\n",
    "                module = module.to(torch.float32)\n",
    "            if \"lm_head\" in name or \"embed_tokens\" in name:\n",
    "                if hasattr(module, \"weight\"):\n",
    "                    module = module.to(torch.float32)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Instructional Chatbots in Spanish\n",
    "\n",
    "In this tutorial, we'll explore how to train instructional chatbots in Spanish using the [somos-clean-alpaca](https://huggingface.co/datasets/somosnlp/somos-clean-alpaca-es) dataset. This dataset provides a rich collection of conversational and instructional interactions in Spanish, making it an ideal resource for developing chatbots capable of understanding and executing specific instructions. We'll leverage the `autotransformers` library to streamline the training process, applying advanced techniques such as LoRA and quantization for efficient model adaptation and performance. Whether you're looking to enhance an existing chatbot or build a new one from scratch, this guide will equip you with the knowledge and tools needed to succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "\n",
    "Before we begin, it's essential to import the necessary libraries that will be used throughout the tutorial. These libraries provide the foundational tools required for loading datasets, configuring models, and training. Below is a brief overview of each import and its role in our project:\n",
    "\n",
    "- `from autotransformers import AutoTrainer, DatasetConfig, ModelConfig`: Imports the `AutoTrainer` class for orchestrating the training process, and `DatasetConfig` and `ModelConfig` for configuring the dataset and model parameters, respectively, within the `autotransformers` library.\n",
    "\n",
    "- `from autotransformers.llm_templates import QLoraWrapperModelInit, modify_tokenizer, qlora_config, SavePeftModelCallback`: These imports from the `autotransformers` library's large language model (LLM) templates module include utilities for initializing models with LoRA wrappers, modifying tokenizers to fit our task, configuring quantization (QLoRA), and implementing a callback to save PEFT (Post-training Efficiency Fine-tuning) models.\n",
    "\n",
    "- `from functools import partial`: The `partial` function from the `functools` module is used to partially apply functions, allowing us to pre-specify some arguments of a function, which is particularly useful for customizing our tokenizer modification function.\n",
    "\n",
    "- `from peft import LoraConfig`: Imports the `LoraConfig` class from the `peft` library, which is used to specify configurations for LoRA (Low-Rank Adaptation), an efficient method for adapting pre-trained models to new tasks with minimal computational overhead.\n",
    "\n",
    "- `from datasets import load_dataset`: From the `datasets` library, we import the `load_dataset` function, which is used to load and preprocess data from a wide range of datasets available in the Hugging Face Datasets repository, including our target dataset `somos-clean-alpaca-es`.\n",
    "\n",
    "Ensure all these libraries are installed in your environment before proceeding with the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a912f00c6eaa4481bfc2c9a853681cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5806c84dc3024f1284be799d7d45ef6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autotransformers import AutoTrainer, DatasetConfig, ModelConfig\n",
    "from autotransformers.llm_templates import instructions_to_chat, NEFTuneTrainer, modify_tokenizer, SavePeftModelCallback\n",
    "from functools import partial\n",
    "from peft import LoraConfig, LoftQConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Chat Template\n",
    "\n",
    "To correctly format the conversations for training, we define a chat template using Jinja2 templating syntax. This template iterates through each message in a conversation, categorizing and formatting them based on their role:\n",
    "\n",
    "- **User Messages**: Wrapped with `<user>` tags to clearly indicate messages from the user. These are the instructions or queries directed at the chatbot.\n",
    "\n",
    "- **System Messages**: Enclosed within `<system>` tags, followed by line breaks for readability. These messages might include system-generated instructions or context that guides the chatbot's responses.\n",
    "\n",
    "- **Assistant Responses**: Placed between the conversation, after `</user>` tags and marked with `</assistant>` tags at the end, along with the end-of-sentence (EOS) token. These are the chatbot's replies or actions taken in response to the user's message, at each utterance or intervention in the conversation.\n",
    "\n",
    "- **Input Data**: Marked with `<input>` tags to distinguish any additional input or contextual information provided to the chatbot.\n",
    "\n",
    "This structured format is crucial for the model to understand the different components of a conversation, enabling it to generate appropriate responses based on the role of each message.\n",
    "\n",
    "Typically, a conversation will start with the system message, then have an input containing additional context for the assistant, and then turns of user-assistant, which can be one or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "The dataset preparation phase is crucial for structuring the data in a way that's conducive to training a chatbot. We first load the dataset from the hub and then utilize `instructions_to_chat`, to transform each sample from the `somos-clean-alpaca` dataset into a format that mirrors a real conversation flow involving a system message, user input, and assistant response.\n",
    "\n",
    "### The `instructions_to_chat` Function\n",
    "\n",
    "`instructions_to_chat` takes a dictionary representing a single dataset sample and restructures it by categorizing and ordering messages based on their role in a conversation:\n",
    "\n",
    "- It starts by adding a **system message** that sets the context for the chatbot as an assistant designed to follow user instructions.\n",
    "- If present, **input data** is added next to provide additional context or information needed to fulfill the user's request.\n",
    "- The **user's instruction** is then added, followed by the **assistant's output**, which is the response to the user's request.\n",
    "\n",
    "This restructuring results in a `messages` list within the sample dictionary, containing all conversation elements in their logical order.\n",
    "\n",
    "### Applying the Transformation\n",
    "\n",
    "To apply this transformation across the entire dataset:\n",
    "\n",
    "- We use the `.map` method with `instructions_to_chat` as the mapping function, setting `batched=False` to process samples individually and `num_proc=4` to parallelize the operation, enhancing efficiency.\n",
    "- Columns not part of the `messages` structure are removed to streamline the dataset.\n",
    "\n",
    "Finally, the dataset is split into training and test sets with a 20% test size, ensuring that we can evaluate our chatbot's performance on unseen data. This split is achieved using the `train_test_split` method, providing a solid foundation for training and validating the chatbot model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab92a4fefba4aeaa415e694107876e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/343 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.30M/2.30M [00:00<00:00, 9.90MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ca09e60b444cc882efc594cdb8e13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"somosnlp/Conversaciones_terapeuticas_espanol\", split= \"train\")\n",
    "ds = ds.rename_column(\"chat\", \"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ds[\"train\"].train_test_split(0.2, seed=203984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Dataset for AutoTransformers\n",
    "\n",
    "To ensure our instructional chatbot model trains efficiently and effectively, we meticulously configure our dataset using the `autotransformers` library's `DatasetConfig`. This step is essential for tailoring the training process to our specific needs, including hyperparameter settings, dataset particulars, and training strategies.\n",
    "\n",
    "### Setting Up Training Arguments\n",
    "\n",
    "A set of fixed training arguments (`fixed_train_args`) is defined to control various aspects of the training process:\n",
    "\n",
    "- **Batch sizes** for both training and evaluation are set to 1, indicating that samples are processed individually. This can be particularly useful for large models or when GPU memory is limited.\n",
    "- **Gradient accumulation** is used with 16 steps, allowing us to effectively simulate a larger batch size and stabilize training without exceeding memory limits.\n",
    "- A **warmup ratio** of 0.03 gradually increases the learning rate at the beginning of training to prevent the model from converging too quickly to a suboptimal solution.\n",
    "- **Learning rate**, **weight decay**, and other optimization settings are carefully chosen to balance model learning speed and quality.\n",
    "- **Evaluation and saving strategies** are configured to periodically check the model's performance and save checkpoints, enabling monitoring and continuation of training from the last saved state.\n",
    "\n",
    "### Crafting the Dataset Configuration\n",
    "\n",
    "The `alpaca_config` dictionary encompasses all necessary information for dataset preparation and integration:\n",
    "\n",
    "- **Dataset details** such as name, task type, and specific columns to use for text and labels ensure that the model trains on the correct data format.\n",
    "- **Training parameters** are included via the `fixed_training_args` dictionary.\n",
    "- **Callback classes**, such as `SavePeftModelCallback`, automate important steps like model saving during training.\n",
    "- **Process optimizations** like setting a seed for reproducibility, specifying the optimization direction and metric, and enabling partial splits for validation set creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "fixed_train_args = {\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"fp16\": True,\n",
    "    \"logging_steps\": 50,\n",
    "    \"lr_scheduler_type\": \"constant\",\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"eval_steps\": 200,\n",
    "    \"save_steps\": 50,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"logging_first_step\": True,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"max_grad_norm\": 0.3,\n",
    "    \"optim\": \"paged_adamw_32bit\",\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"group_by_length\": False,\n",
    "    \"save_total_limit\": 50,\n",
    "    \"adam_beta2\": 0.999\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fixed_train_args = {\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"fp16\": True,  # Cambiado de True a False\n",
    "    \"logging_steps\": 50,\n",
    "    \"lr_scheduler_type\": \"constant\",\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"eval_steps\": 200,\n",
    "    \"save_steps\": 50,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"logging_first_step\": True,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"max_grad_norm\": 0.3,\n",
    "    \"optim\": \"paged_adamw_32bit\",\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"group_by_length\": False,\n",
    "    \"save_total_limit\": 50,\n",
    "    \"adam_beta2\": 0.999\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config = {\n",
    "        \"seed\": 9834,\n",
    "        \"direction_optimize\": \"minimize\",\n",
    "        \"metric_optimize\": \"eval_loss\",\n",
    "        \"callbacks\": [SavePeftModelCallback],\n",
    "        \"fixed_training_args\": fixed_train_args,\n",
    "        \"dataset_name\": \"Conversaciones_terapeuticas_espanol\",\n",
    "        \"alias\": \"terapia\",\n",
    "        \"retrain_at_end\": False,\n",
    "        \"task\": \"chatbot\",\n",
    "        \"text_field\": \"messages\",\n",
    "        \"label_col\": \"messages\",\n",
    "        \"num_proc\": 4,\n",
    "        \"loaded_dataset\": ds,\n",
    "        \"partial_split\": True, # to create a validation split.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config = DatasetConfig(**model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "In the \"Model Configuration\" section, we outline how to set up the model configurations using `autotransformers`, focusing on integrating LoRA (Low-Rank Adaptation) for model adaptation and applying quantization for efficiency. These steps are crucial for tailoring the model to our specific task and environment, ensuring optimal performance and resource utilization.\n",
    "\n",
    "### LoRA Configuration\n",
    "\n",
    "The `LoraConfig` object is instantiated with parameters tailored to enhance model adaptability while maintaining efficiency:\n",
    "\n",
    "- **r (rank)** and **lora_alpha** are set to adjust the capacity and learning rate multiplier for LoRA layers, balancing between model flexibility and overfitting risk.\n",
    "- **target_modules** specifies which parts of the model to apply LoRA. In this case, \"all-linear\" modules are targeted for adaptation, offering a broad enhancement over the model's capabilities.\n",
    "- **lora_dropout** is adjusted based on the model size, ensuring that regularization is appropriately scaled.\n",
    "- **bias** configuration is set to \"none\", indicating that no additional bias terms are used in the LoRA adaptation layers.\n",
    "- The **task_type** is specified as \"CAUSAL_LM\" to indicate the causal language modeling task, aligning with the instructional chatbot's nature.\n",
    "\n",
    "### GEMMA Model Configuration\n",
    "\n",
    "The `ModelConfig` for the GEMMA model includes several key parameters and customizations:\n",
    "\n",
    "- **Model Name**: Specifies the pre-trained model to be adapted, \"google/gemma-2b-it\" in this case.\n",
    "- **Save Name and Directory**: Defines the naming convention and location for saving the fine-tuned model.\n",
    "- **Custom Parameters**: Includes model-specific settings, such as enabling trust in remote code and configuring device mapping for training.\n",
    "- **Model Initialization Wrapper**: `QLoraWrapperModelInit` is used to integrate the QLoRA quantization framework with the LoRA-configured model, optimizing for both adaptability and efficiency.\n",
    "- **Quantization and PEFT Configurations**: These are applied via the `quantization_config` and `peft_config` parameters, ensuring that the model benefits from both LoRA adaptations and efficient post-training quantization.\n",
    "- **Tokenizer Modification**: A partial function is used to customize the tokenizer, adjusting sequence length, adding special tokens, and incorporating the chat template designed for our conversational context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=256,\n",
    "        lora_alpha=32,\n",
    "        target_modules=\"all-linear\",  # \"query_key_value\" # \"Wqkv\"\n",
    "        lora_dropout=0.1,  # 0.1 for <13B models, 0.05 otherwise.\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        use_rslora=True,\n",
    "        loftq_config=LoftQConfig(loftq_bits=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "funciona_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_config = ModelConfig(\n",
    "    name=\"google/gemma-2b-it\",\n",
    "    save_name=\"gemma_2b\",\n",
    "    save_dir=\"./gemma_2b_terapia_1\",\n",
    "    custom_params_model={\"trust_remote_code\": True, \"device_map\": {\"\": 0}},\n",
    "    model_init_wrap_cls=QLoraWrapperModelInit,\n",
    "    quantization_config=funciona_config,\n",
    "    peft_config=lora_config,\n",
    "    # neftune_noise_alpha=10,\n",
    "    # custom_trainer_cls=NEFTuneTrainer,\n",
    "    func_modify_tokenizer=partial(\n",
    "        modify_tokenizer,\n",
    "        new_model_seq_length=4096, # lower the maximum seq length to 4096 instead of 8192 to fit in google colab GPUs.\n",
    "        add_special_tokens={\"pad_token\": \"[PAD]\"}, # add pad token.\n",
    "        chat_template=CHAT_TEMPLATE # add the new chat template including the system and input roles.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Train\n",
    "\n",
    "With our dataset and model configurations in place, we're now ready to initiate the training process. This is where the `AutoTrainer` class from the `autotransformers` library comes into play, orchestrating the entire training operation based on the specifications we've provided.\n",
    "\n",
    "### Setting Up the AutoTrainer\n",
    "\n",
    "The `AutoTrainer` is a comprehensive class designed to streamline the training of machine learning models, especially tailored for large language models. It accepts several parameters to control the training process:\n",
    "\n",
    "- **Model Configurations**: A list of `ModelConfig` objects, each defining the settings and customizations for a model. For our instructional chatbot, we include the configuration for the GEMMA model adapted with LoRA and quantization.\n",
    "- **Dataset Configurations**: Similar to model configurations, these are specified using `DatasetConfig` objects. We pass the configuration for our pre-processed and structured `alpaca` dataset, ensuring it's utilized effectively during training.\n",
    "- **Metrics Directory**: Specifies the directory where training metrics will be stored, allowing for performance monitoring and evaluation.\n",
    "- **Hyperparameter Search Mode**: Set to \"fixed\" in our case, indicating that we're not exploring different hyperparameters but rather training with a predetermined set.\n",
    "- **Clean**: A boolean flag to clean any previous runs' data, ensuring a fresh start for each training session.\n",
    "- **Metrics Cleaner**: Specifies the utility for handling temporary metrics data, keeping our metrics directory tidy and focused on significant results.\n",
    "- **Use Auth Token**: Enables the use of an authentication token, necessary for accessing certain models or datasets that may have access restrictions.\n",
    "\n",
    "### Initiating the Training\n",
    "\n",
    "With the `AutoTrainer` configured, we proceed to call its execution method. This step starts the training process, leveraging the configurations we've meticulously set up. The process involves:\n",
    "\n",
    "- Automatically loading and preparing the dataset according to our `DatasetConfig`.\n",
    "- Adapting and fine-tuning the model based on the `ModelConfig`, including any specified LoRA or quantization enhancements.\n",
    "- Regularly evaluating the model's performance using the provided validation set, allowing us to monitor its effectiveness in real-time.\n",
    "- Saving model checkpoints and training metrics, enabling both introspection of the training process and the resumption of training from the last saved state.\n",
    "\n",
    "Upon completion, the training results, including performance metrics and model checkpoints, are made available for analysis and deployment. This step marks the culmination of our instructional chatbot's preparation, rendering it ready for testing and eventually, deployment in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autotrainer = AutoTrainer(\n",
    "    model_configs=[gemma_config],\n",
    "    dataset_configs=[model_config],\n",
    "    metrics_dir=\"./chaterapia\",\n",
    "    hp_search_mode=\"fixed\",\n",
    "    clean=True,\n",
    "    metrics_cleaner=\"tmp_metrics_cleaner\",\n",
    "    use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating over datasets...:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Trying models on dataset Conversaciones_terapeuticas_espanol:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa85617d096845628b04b2a14075b8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3a2dee856140ccade21befe440a135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ebf591710048a6b16dec3170af3533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1614b7170f37437ab9a3ba2528af654c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/888 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae11b0dec85f426ea4253de368dd3b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f4710bdc5940829733ac3879142473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89309d5184b4c638e62b15aaec6e8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3675853ff3264ee6ac0abe05db38ff0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285dfd94acde4a6b9b5d2327c40d59a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efd49948b704c43926165e232c633fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd418af5a8ea4ef9b35b8491cd83671c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404bd7b8026849c6b3fb32360602df3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c381e4fe3294b4c945d180fcdc79368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b9c6aa344b41fd82601218096ec2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 09:41, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving PEFT checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 00:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying models on dataset Conversaciones_terapeuticas_espanol: 100%|██████████| 1/1 [12:14<00:00, 734.30s/it]\u001b[A\n",
      "Iterating over datasets...: 100%|██████████| 1/1 [12:14<00:00, 734.30s/it]\n"
     ]
    }
   ],
   "source": [
    "result = autotrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer/tokenizer_config.json',\n",
       " './tokenizer/special_tokens_map.json',\n",
       " './tokenizer/tokenizer.model',\n",
       " './tokenizer/added_tokens.json',\n",
       " './tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autotrainer.tokenizer.save_pretrained(\"./tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b13d2b119da4dcd897b582560a9b06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/5.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Juliofc/chaterapia_model/commit/eebca98fca5e53f648195a0513ef9447922c8160', commit_message='Upload adapter_model.safetensors with huggingface_hub', commit_description='', oid='eebca98fca5e53f648195a0513ef9447922c8160', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import upload_file,HfFolder\n",
    "\n",
    "# Define tus variables\n",
    "path_to_file = \"gemma_2b_terapia_1/fixedparams_gemma_2b-terapia/checkpoint-38/adapter_model/adapter_model.safetensors\"  # Ruta local al archivo que quieres subir\n",
    "path_in_repo = \"adapter_model.safetensors\"  # Ruta en el repositorio donde se guardará el archivo\n",
    "repo_id = \"Juliofc/chaterapia_model\"  # \"usuario/nombre_del_repositorio\"\n",
    "\n",
    "# Subir archivo\n",
    "upload_file(\n",
    "    path_or_fileobj=path_to_file,\n",
    "    path_in_repo=path_in_repo,\n",
    "    repo_id=repo_id,\n",
    "    token=HfFolder.get_token(),  # Asegúrate de que tu token está correctamente configurado\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Juliofc/chaterapia_model/commit/0eb9cda4e72789195b46ecdbdb3fc5b4089e7f7c', commit_message='Upload adapter_config.json with huggingface_hub', commit_description='', oid='0eb9cda4e72789195b46ecdbdb3fc5b4089e7f7c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tus variables\n",
    "path_to_file = \"gemma_2b_terapia_1/fixedparams_gemma_2b-terapia/checkpoint-38/adapter_model/adapter_config.json\"  # Ruta local al archivo que quieres subir\n",
    "path_in_repo = \"config.json\"  # Ruta en el repositorio donde se guardará el archivo\n",
    "repo_id = \"Juliofc/chaterapia_model\"  # \"usuario/nombre_del_repositorio\"\n",
    "\n",
    "# Subir archivo\n",
    "upload_file(\n",
    "    path_or_fileobj=path_to_file,\n",
    "    path_in_repo=path_in_repo,\n",
    "    repo_id=repo_id,\n",
    "    token=HfFolder.get_token(),  # Asegúrate de que tu token está correctamente configurado\n",
    ")\n",
    "\n",
    "path_in_repo = \"adapter_config.json\"  # Ruta en el repositorio donde se guardará el archivo\n",
    "repo_id = \"Juliofc/chaterapia_model\"  # \"usuario/nombre_del_repositorio\"\n",
    "\n",
    "# Subir archivo\n",
    "upload_file(\n",
    "    path_or_fileobj=path_to_file,\n",
    "    path_in_repo=path_in_repo,\n",
    "    repo_id=repo_id,\n",
    "    token=HfFolder.get_token(),  # Asegúrate de que tu token está correctamente configurado\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Juliofc/chaterapia_model/commit/14b13394452f67245188aa4e6595db652502f942', commit_message='Upload README.md with huggingface_hub', commit_description='', oid='14b13394452f67245188aa4e6595db652502f942', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tus variables\n",
    "path_to_file = \"gemma_2b_terapia_1/fixedparams_gemma_2b-terapia/checkpoint-38/adapter_model/README.md\"  # Ruta local al archivo que quieres subir\n",
    "path_in_repo = \"README.md\"  # Ruta en el repositorio donde se guardará el archivo\n",
    "repo_id = \"Juliofc/chaterapia_model\"  # \"usuario/nombre_del_repositorio\"\n",
    "\n",
    "# Subir archivo\n",
    "upload_file(\n",
    "    path_or_fileobj=path_to_file,\n",
    "    path_in_repo=path_in_repo,\n",
    "    repo_id=repo_id,\n",
    "    token=HfFolder.get_token(),  # Asegúrate de que tu token está correctamente configurado\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer/tokenizer_config.json\n",
      "tokenizer/special_tokens_map.json\n",
      "tokenizer/added_tokens.json\n",
      "tokenizer/tokenizer.model\n",
      "tokenizer/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define la ruta de la carpeta de la cual quieres obtener los archivos\n",
    "carpeta = 'tokenizer'\n",
    "\n",
    "# Lista para almacenar las rutas completas de los archivos\n",
    "rutas_archivos = []\n",
    "\n",
    "# Recorre los archivos en el directorio\n",
    "for raiz, dirs, archivos in os.walk(carpeta):\n",
    "    for nombre_archivo in archivos:\n",
    "        ruta_completa = os.path.join(raiz, nombre_archivo)\n",
    "        rutas_archivos.append(ruta_completa)\n",
    "\n",
    "# Imprime la lista de rutas de archivos\n",
    "for ruta in rutas_archivos:\n",
    "    print(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in rutas_archivos:\n",
    "    # Define tus variables\n",
    "    path_to_file = i  # Ruta local al archivo que quieres subir\n",
    "    path_in_repo = i  # Ruta en el repositorio donde se guardará el archivo\n",
    "    repo_id = \"Juliofc/chaterapia_model\"  # \"usuario/nombre_del_repositorio\"\n",
    "\n",
    "    # Subir archivo\n",
    "    upload_file(\n",
    "        path_or_fileobj=path_to_file,\n",
    "        path_in_repo=path_in_repo,\n",
    "        repo_id=repo_id,\n",
    "        token=HfFolder.get_token(),  # Asegúrate de que tu token está correctamente configurado\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
